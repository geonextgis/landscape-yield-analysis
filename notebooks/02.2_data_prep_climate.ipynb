{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f52c5a2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8891756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from glob import glob\n",
    "import warnings\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.stats import weibull_min\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "# Suppress warnings to keep output clean\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c97c24",
   "metadata": {},
   "source": [
    "## Define the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd67193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP = 'ww'\n",
    "DISTANCE = 2.5\n",
    "EPSG = 25832\n",
    "\n",
    "# Path of the directories\n",
    "WORK_DIR = '/beegfs/halder/GITHUB/RESEARCH/landscape-yield-analysis/'\n",
    "os.chdir(WORK_DIR)\n",
    "MAIN_DATA_DIR = '/beegfs/halder/DATA/'\n",
    "WORK_DATA_DIR = os.path.join(WORK_DIR, 'data')\n",
    "WORK_TEMP_DIR = os.path.join(WORK_DIR, 'temp')\n",
    "\n",
    "# Define the DWD data directory\n",
    "DWD_DATA_DIR = '/beegfs/common/data/climate/dwd/csvs/germany_ubn_1951-01-01_to_2024-08-30'\n",
    "\n",
    "OUT_DIR = os.path.join(WORK_DIR, 'output', str(DISTANCE), CROP)\n",
    "if os.path.exists(OUT_DIR) == False:\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print('Output directory successfully created!')\n",
    "else:\n",
    "    print('Output directory already existed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7fe161",
   "metadata": {},
   "source": [
    "## Load Hexagonal Grid for Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to grid shapefile\n",
    "GRID_PATH = os.path.join(WORK_DATA_DIR, 'VECTOR', f'DE_Hexbins_{DISTANCE}sqkm_EPSG_{EPSG}.shp')\n",
    "\n",
    "# Load grid as a GeoDataFrame and retain relevant columns\n",
    "grids_gdf = gpd.read_file(GRID_PATH)\n",
    "grids_gdf = grids_gdf[['id', 'geometry']]\n",
    "grids_gdf['id'] = grids_gdf['id'].astype(int)\n",
    "\n",
    "print('Successfully read the grids!')\n",
    "grids_gdf.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Germany shapefile\n",
    "DE_gdf = gpd.read_file(os.path.join(MAIN_DATA_DIR, 'DE_NUTS', 'DE_NUTS_3.shp'))\n",
    "DE_gdf = DE_gdf[DE_gdf['LEVL_CODE']==1] \n",
    "DE_gdf = DE_gdf.to_crs(f'EPSG:{EPSG}')\n",
    "\n",
    "grids_centroids = grids_gdf.copy()\n",
    "grids_centroids['geometry'] = grids_gdf.centroid\n",
    "\n",
    "grids_centroids = gpd.sjoin_nearest(left_df=grids_centroids, right_df=DE_gdf[['NUTS_NAME', 'geometry']],\n",
    "                                    how='inner')\n",
    "\n",
    "grids_gdf = pd.merge(left=grids_gdf, right=grids_centroids[['id', 'NUTS_NAME']], how='inner', on='id')\n",
    "\n",
    "# Specify the DE DWD grip file path\n",
    "DE_DWD_json_path = os.path.join(MAIN_DATA_DIR, 'DE_DWD_Lat_Lon', 'latlon_to_rowcol.json')\n",
    "\n",
    "with open(DE_DWD_json_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert data to GeoDataFrame\n",
    "records = []\n",
    "for coord, index in data:\n",
    "    lat, lon = coord\n",
    "    row, col = index\n",
    "    point = Point(lon, lat)\n",
    "    records.append({'row': row, 'col': col, 'geometry': point})\n",
    "\n",
    "latlon_gdf = gpd.GeoDataFrame(records, geometry='geometry', crs='EPSG:4326')\n",
    "latlon_gdf = latlon_gdf.to_crs(f'EPSG:{EPSG}')\n",
    "\n",
    "# Apply spatial join\n",
    "grids_gdf_dwd = gpd.sjoin(left_df=grids_gdf, right_df=latlon_gdf, how='inner', predicate='intersects')\n",
    "grids_gdf_dwd.drop(columns=['index_right', 'geometry'], inplace=True)\n",
    "\n",
    "for col in ['id', 'row', 'col']:\n",
    "    grids_gdf_dwd[col] = grids_gdf_dwd[col].astype(int)\n",
    "\n",
    "grids_gdf_dwd['rowcol'] = list(zip(grids_gdf_dwd['row'], grids_gdf_dwd['col']))\n",
    "\n",
    "print(grids_gdf_dwd.shape)\n",
    "grids_gdf_dwd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38a6e0",
   "metadata": {},
   "source": [
    "## Process the phenolgy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea89b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the phenology data\n",
    "phenology_df = pd.read_csv(os.path.join(MAIN_DATA_DIR, 'DE_Crop_Phenology', f'{CROP}_phenology.csv'))\n",
    "phenology_df.rename(columns={'Sowing_DOY': 'Sowing_DATE', 'Flowering_DOY': 'Flowering_DATE', 'Harvest_DOY': 'Harvest_DATE'}, inplace=True)\n",
    "for date_col in ['Sowing_DATE', 'Flowering_DATE', 'Harvest_DATE']:\n",
    "    event = date_col.split(\"_\")[0]\n",
    "    phenology_df[date_col] = pd.to_datetime(phenology_df[date_col], format='%Y-%m-%d')\n",
    "    phenology_df[f'{event}_DOY'] = phenology_df[date_col].dt.dayofyear\n",
    "\n",
    "# Group by STATE_ID and take the median of DOYs\n",
    "median_doys = phenology_df.groupby('STATE_NAME')[['Sowing_DOY', 'Harvest_DOY']].median().round().astype(int).reset_index()\n",
    "median_doys['Start_DOY'] = median_doys['Sowing_DOY'].apply(lambda v: v - 30 if v > 30 else 365 + (v - 30))\n",
    "median_doys['End_DOY'] = median_doys['Harvest_DOY']\n",
    "\n",
    "phenology_df = phenology_df.merge(median_doys, on='STATE_NAME', suffixes=('', '_MEDIAN'))\n",
    "\n",
    "# Create full year range\n",
    "years = list(range(1952, 2024))\n",
    "\n",
    "# Create cartesian product of states and years\n",
    "phenology_median = pd.MultiIndex.from_product([median_doys['STATE_NAME'], years], names=['STATE_NAME', 'Year']).to_frame(index=False)\n",
    "\n",
    "# Merge back state names and median DOYs\n",
    "phenology_median = phenology_median.merge(median_doys, on='STATE_NAME', how='left')\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "def calculate_duration(row):\n",
    "    start_doy = row['Start_DOY']\n",
    "    end_doy = row['End_DOY']\n",
    "    year = row['Year']\n",
    "    \n",
    "    if end_doy >= start_doy:\n",
    "        return end_doy - start_doy\n",
    "    else:\n",
    "        # Check for leap year (i.e. next year)\n",
    "        next_year = year + 1\n",
    "        is_leap = (next_year % 4 == 0 and (next_year % 100 != 0 or next_year % 400 == 0))\n",
    "        year_length = 366 if is_leap else 365\n",
    "        return (year_length - start_doy) + end_doy\n",
    "\n",
    "phenology_median['Duration'] = phenology_median.apply(calculate_duration, axis=1)\n",
    "\n",
    "def doy_to_date(year, doy, event='sow'):\n",
    "    if event == 'sow':\n",
    "        return datetime(year-1, 1, 1) + timedelta(days=int(doy) - 1)\n",
    "    else:\n",
    "        return datetime(year, 1, 1) + timedelta(days=int(doy) - 1)\n",
    "\n",
    "phenology_median['Sowing_DATE'] = phenology_median.apply(lambda row: doy_to_date(row['Year'], row['Sowing_DOY'], event='sow'), axis=1)\n",
    "phenology_median['Harvest_DATE'] = phenology_median.apply(lambda row: doy_to_date(row['Year'], row['Harvest_DOY'], event='harvest'), axis=1)\n",
    "\n",
    "phenology_median['Start_DATE'] = phenology_median.apply(lambda row: doy_to_date(row['Year'], row['Start_DOY'], event='sow'), axis=1)\n",
    "phenology_median['End_DATE'] = phenology_median.apply(lambda row: doy_to_date(row['Year'], row['End_DOY'], event='harvest'), axis=1)\n",
    "\n",
    "print(phenology_median.shape)\n",
    "phenology_median.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7720b",
   "metadata": {},
   "source": [
    "## Process the climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f3af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phenology(state_name, year):\n",
    "    index = phenology_median[(phenology_median['STATE_NAME']==state_name) & (phenology_median['Year']==year)].index[0]\n",
    "    data_dict = phenology_median.loc[index].to_dict()\n",
    "    return data_dict\n",
    "\n",
    "def extract_climate_data_by_grids(hex_id, out_dir):\n",
    "    state_name = grids_gdf[grids_gdf['id'] == hex_id]['NUTS_NAME'].iloc[0]\n",
    "    row_cols = np.array(grids_gdf_dwd[grids_gdf_dwd['id'] == hex_id]['rowcol'])\n",
    "\n",
    "    # Prepare list of valid climate files\n",
    "    file_paths = []\n",
    "    for row, col in row_cols:\n",
    "        fpath = os.path.join(DWD_DATA_DIR, str(row), f'daily_mean_RES1_C{col}R{row}.csv.gz')\n",
    "        if os.path.exists(fpath):\n",
    "            file_paths.append(fpath)\n",
    "\n",
    "    # Read all valid climate files into a dict of DataFrames\n",
    "    clim_dfs = []\n",
    "    for f in file_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(f, delimiter='\\t', usecols=['Date', 'Precipitation', 'TempMin', 'TempMax', 'Radiation'])\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            clim_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {f}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not clim_dfs:\n",
    "        print(f\"No climate files found for hex ID {hex_id}\")\n",
    "        return\n",
    "\n",
    "    # Merge all DataFrames by taking mean per day\n",
    "    clim_merged = pd.concat(clim_dfs).groupby('Date').mean().reset_index()\n",
    "\n",
    "    # Loop through years\n",
    "    for year in sorted(phenology_median['Year'].unique()):\n",
    "        pheno = get_phenology(state_name, year)\n",
    "        if pheno is None:\n",
    "            continue\n",
    "\n",
    "        start_date = pheno['Sowing_DATE']\n",
    "        end_date = pheno['Harvest_DATE']\n",
    "\n",
    "        # Extract climate data for the phenological window\n",
    "        clim_year = clim_merged[\n",
    "            (clim_merged['Date'] >= start_date) &\n",
    "            (clim_merged['Date'] <= end_date)\n",
    "        ].copy()\n",
    "\n",
    "        if clim_year.empty:\n",
    "            continue\n",
    "\n",
    "        # Save to file\n",
    "        save_path = os.path.join(out_dir, f'{hex_id}_{year}.csv')\n",
    "        clim_year.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9af21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR_climate = os.path.join(OUT_DIR, 'climate')\n",
    "\n",
    "if os.path.exists(OUT_DIR_climate):\n",
    "    print('Directory already exists!')\n",
    "else:\n",
    "    os.makedirs(OUT_DIR_climate, exist_ok=True)\n",
    "    print('Directory successfully created!')\n",
    "\n",
    "# Prepare all hex_ids to be processed\n",
    "hex_ids = grids_gdf_dwd['id'].unique()\n",
    "\n",
    "# Function to wrap\n",
    "def wrapper(hex_id):\n",
    "    extract_climate_data_by_grids(hex_id, OUT_DIR_climate)\n",
    "\n",
    "# Use tqdm's process_map instead of executor.map\n",
    "process_map(wrapper, hex_ids, max_workers=70, chunksize=1)\n",
    "print('Climate data computation complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
